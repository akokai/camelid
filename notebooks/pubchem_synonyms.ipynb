{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Accessing PubChem Compound synonyms via FTP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "# import gzip\n",
    "import mmap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download and uncompress the filtered CID-synonym file\n",
    "\n",
    "**Warning: Compressed file is ~1 GB. Uncompressed file > 4 GB.**\n",
    "\n",
    "- Download `CID-Synonym-unfiltered.gz` from [here](ftp://ftp.ncbi.nlm.nih.gov/pubchem/Compound/Extras/) yourself, or run from command line: `wget -b ftp://ftp.ncbi.nlm.nih.gov/pubchem/Compound/Extras/CID-Synonym-filtered.gz`\n",
    "\n",
    "- To unzip: `gzip -d CID-Synonym-unfiltered.gz` (use `-dk` to avoid deleting the `.gz` file)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# This is where I am keeping the file on my machine.\n",
    "DATA_PATH = os.path.join(os.getenv('CAMELID_HOME'), 'synonyms', 'data')\n",
    "synonyms_gz = os.path.join(DATA_PATH, 'CID-Synonym-filtered.gz')\n",
    "synonyms = os.path.join(DATA_PATH, 'CID-Synonym-filtered')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parse needed information out of the synonyms file\n",
    "\n",
    "From the README:\n",
    "\n",
    "    These are listings of all names associated with a CID. The\n",
    "    unfiltered list are names aggregated from all SIDs whose \n",
    "    standardized form is that CID, sorted by weight with the \"best\"\n",
    "    names first. The filtered list has some names removed that are\n",
    "    considered inconsistend with the structure. Both are gzipped text\n",
    "    files with CID, tab, and name on each line. Note that the\n",
    "    names may be composed of more than one word, separated by spaces.\n",
    "\n",
    "\n",
    "The uncompressed file exceeds my disk quota on the OCF server, so I would like to keep it compressed. It's possible to read compressed data in Python using `gzip.open()`. But the data are then accessed as `bytes`, rather than `str` as with the regular `open()`. \n",
    "\n",
    "Additionally, we don't want to load the whole 1 GB file into RAM, so we want to either read it incrementally, or use some other method of accessing it, such as a memory map (`mmap` from the standard library). We'll also probably want to transform the data that we extract into another format on disk (JSONL, CSV, database) rather than storing it as a Python object in memory.\n",
    "\n",
    "There are some interesting potential solutions in [this Stack Overflow post](http://stackoverflow.com/questions/6219141/searching-for-a-string-in-a-large-text-file-profiling-various-methods-in-pytho) and [this one](http://codereview.stackexchange.com/questions/78224/optimize-huge-text-file-search)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Strategy 1: Regex on compressed file\n",
    "\n",
    "Can we make re.findall() work on a `bytes`-type object via `mmap`?\n",
    "\n",
    "Test the regex part in a small example dataset..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[b'THF', b'tetrahydrofuran']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cid = '8028'  # THF, should be in the file\n",
    "\n",
    "# Yes, this is silly with the regex notation AND the format notation, but it works...\n",
    "search_rx = r'(?:{})\\t(.*)'.format(cid).encode('utf-8')\n",
    "p = re.compile(search_rx)\n",
    "\n",
    "text = \"\"\"\n",
    "8008\\tHi\n",
    "8028\\tTHF\n",
    "8028\\ttetrahydrofuran\n",
    "8080\\tdrums\n",
    "\"\"\".encode('utf-8')\n",
    "\n",
    "re.findall(p, text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try it on the actual file. Using `mmap` to be able to randomly access the file without loading it into memory (how does this work anyway?)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\tAcetyl-DL-carnitine\n",
      "\n",
      "[b'Acetyl-DL-carnitine']\n",
      "b\"s\\r\\xf5\\x83\\tf=nd\\x9d\\x13\\xdfJ\\t\\x1a?c\\x1a\\xa6\\x16\\xcf\\x98\\x03y\\x1d\\x86\\xb8\\xa5[\\xcf\\xd1\\xdcS\\x89~_\\xee\\x11\\xcf\\x8f\\x16\\x18\\xb3G\\xf4\\xa2\\x1b\\x88\\xcb\\xf6\\xe2JH')\\xe6\\x82Gv#\\x91\\xc2D\\xa7\\x92\\xf8\\xb2@7\\xb6\\xdd\\xdd\\xa5\\x8c\\xe6\\x06h\\x90\\x92\\xb2\\xd3\\r\\x10F\\xb3t\\x14a\\xb8\\xb6p\\xa33n(<\\xe8\\x88\\xd2$J\\x83\\xee8\\t\\xee\\xec\\xe2\\x05\\x9b\\x08\"\n"
     ]
    },
    {
     "ename": "UnicodeDecodeError",
     "evalue": "'utf-8' codec can't decode byte 0xf5 in position 2: invalid start byte",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnicodeDecodeError\u001b[0m                        Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-30-9f692f00e7ef>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mitem\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfindall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mUnicodeDecodeError\u001b[0m: 'utf-8' codec can't decode byte 0xf5 in position 2: invalid start byte"
     ]
    }
   ],
   "source": [
    "cid = '1'\n",
    "\n",
    "search_rx = r'(?:{})\\t(.*)'.format(cid).encode('utf-8')\n",
    "p = re.compile(search_rx)\n",
    "\n",
    "with gzip.open(synonyms_gz, 'rb') as file:\n",
    "    s = file.readline()\n",
    "    print(s.decode())\n",
    "    print(re.findall(p, s))\n",
    "    with mmap.mmap(file.fileno(), 0, access=mmap.ACCESS_READ) as mm:\n",
    "        for item in re.findall(p, mm):\n",
    "            print(item)\n",
    "            print(item.decode())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Problems:\n",
    "-  OK, you can decode the lines of the compressed file one at a time and search each one using `re`. However, you can't search a whole file object all at once unless you read it all into memory (undesirable), or use `mmap`.\n",
    "-  **But** `re.findall()` on the `mmap` object does not work as expected. I think it is memory-mapping the compressed file.\n",
    "    - It finds bytes that can't be decoded as text, and which do not correspond to the search string text.\n",
    "    - Actually, it doesn't find `8028` at all.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Strategy 2: Regex on uncompressed file\n",
    "\n",
    "Works but is slow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def find_synonyms(cid):\n",
    "    search_rx = r'(?:{})\\t(.*)'.format(cid).encode('utf-8')\n",
    "    p = re.compile(search_rx)\n",
    "    res = []\n",
    "\n",
    "    with open(synonyms, 'r') as file, \\\n",
    "        mmap.mmap(file.fileno(), 0, access=mmap.ACCESS_READ) as mm:\n",
    "        for item in re.findall(p, mm):\n",
    "            res.append(item)\n",
    "\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 loop, best of 3: 16.1 s per loop\n"
     ]
    }
   ],
   "source": [
    "%timeit thf = find_synonyms(8028)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[b'TETRAHYDROFURAN',\n",
       " b'Oxolane',\n",
       " b'109-99-9',\n",
       " b'Butylene oxide',\n",
       " b'Furanidine',\n",
       " b'Hydrofuran',\n",
       " b'Furan, tetrahydro-',\n",
       " b'Oxacyclopentane',\n",
       " b'Tetramethylene oxide',\n",
       " b'1,4-Epoxybutane']"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "thf = find_synonyms(8028)\n",
    "thf[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assuming we do this, next task is efficiently aggregating, storing, and further processing the results. SQLite database?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Strategy 3: Read line by line...\n",
    "\n",
    "Brute force. To make this less stupid, we should take advantage of the fact that the CIDs in the synonyms file are sorted (probably lexicographically, since the first CID is `1`). Not sure how exactly to do this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8028\tTETRAHYDROFURAN\n",
      "\n",
      "71609\tMescaline sulfate\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-b7884d5cc3c4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m             \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcids\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Exploratory...\n",
    "\n",
    "cids = ['8028', '71609', '88888888']  # actual CIDs that should be in the file\n",
    "\n",
    "with gzip.open(synonyms_gz, 'rb') as file:\n",
    "    for i in range(len(cids)):\n",
    "        for line in file:\n",
    "            s = line.decode('utf-8')\n",
    "            if s.startswith(cids[i]):\n",
    "                print(s)\n",
    "                break  # The wrong thing to do, but just testing"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:cmld]",
   "language": "python",
   "name": "conda-env-cmld-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
